---
title: "Przyczyny stopniowego zmniejszania się długości śledzi oceanicznych wyławianych w Europie"
author: "Piotr Padykuła"
date: "`r Sys.Date()`"
output: html_document
---


```{r globalSettings, echo=FALSE, message=FALSE,include=F}

library(plyr)
library(dplyr)
library(tidyr)
library(EDAWR)
library(mlbench)
library(caret)
library(pROC)
library(knitr)
library(caTools)
library(ggplot2)
library(e1071)
library(rpart)
library(randomForest)
library(gclus)
require(kernlab)
#require(leaps)
#require(MASS)


knitr::opts_chunk$set(echo = TRUE)
#sapply(list.files(pattern="[.]R$", path="../MyScripts/", full.names=TRUE), source);
sapply(list.files(pattern="DataCleaning.R$", path="../MyScripts/", full.names=TRUE), source);
```

# Streszczenie analizy danych
1. Zastąpienie artykułów brakujących średnią arytmetyczną z wartości porzedniej i następnej.
2. Sprawdzenie korelacji wśród zmiennych
    + Usunąłem zmienne (lcop1,lcop2,fbar), których korelacja była większa od 0.8
3. Zmienne wykorzystane w modelu:  X,cfin1,cfin2,chel1,chel2,recr,cumf,totaln,sst,sal,xmonth,nao
    + Pomijałem zmieną porządkową X, gdyż moim zdaniem kolejność wykonywania obserwacji nie powinna być przyczyną
      wpływającą na zachowanie zmiennej zależnej.
4. Podział zbioru na dane treningowe i testowe
    + Podzieliłem zbiór na dane treningowe i testowe (odpowiednio 0.7 i 0.3 wszystkich obserwac)
5. Wykorzystane modele:
    + Regresja liniowa (wykkorzystywałem podejście oparte o Backword elimination z warunkiem wyjścia p-Value<0.05)
    + Random Forest
    + Gradient Boosting
    + Drzewa decyzyjne
    + SVR-support vector regression
6. Najlepszym algorytmem okazał się Random Forest.
7. Wpływ na zmienną objaśnianą
    + Największy wpływ na zmienną objaśnianą miał czynnik SST (temperatura przy powierzchni wody [°C])
    Śledz zaczął maleć w głównej mierze ze względu na wzrost temperatury przy powierzchni wody.


# Import danych
```{r loadData,include=FALSE,cache=TRUE}
#Import danych z pliku
#df<-read.csv("C:\\MOJE_PLIKI\\Sem3_ZaawEksplDanych\\PROJEKT_2\\sledzie.csv")
data<-read.csv("C:\\MOJE_PLIKI\\Sem3_ZaawEksplDanych\\PROJEKT_2\\sledzie.csv",stringsAsFactors = FALSE,na.strings = "?")
```

# Kompletność danych w zbiorze
```{r completeCases,include=TRUE,cache=TRUE}
str(data)
str(data[complete.cases(data),])
```

# Brakujące wartości
```{r missingValues,include=TRUE,cache=TRUE}
sapply(data,function(x){sum(is.na(x))})
head(data,n=5)
ggplot(data=data,aes(x=X,y=length))+geom_line()
```

# Brakujące wartości zastąpiłem średnią z dwóch wartości sąsiednich tj. (X(n-1)+X(n+1))/2
```{r missingValueReplacement,include=TRUE,cache=TRUE}
mynames<-colnames(data)
checkColumns<-sapply(data,function(x){sum(is.na(x))})
for(i in seq_along(mynames)) 
{
  if(checkColumns[i]>0)
  {
    data[,i]<-replaceUnknown(data[,i])
  }
}
```

# Krótkie podsumowanie danych w zbiorze
```{r dataSummary, cache=TRUE}
data=tbl_df(data)
kable(summary(data))
kable(str(data))
ggplot(data=data)+geom_line(aes(x=X,y=length),color="blue")+
  geom_line(aes(x=X,y=cfin1),color="green")+geom_line(aes(x=X,y=cfin2),color="red")
ggplot(data=data)+geom_line(aes(x=X,y=length),color="blue")+
  geom_line(aes(x=X,y=chel1),color="green")+geom_line(aes(x=X,y=chel2),color="red")
ggplot(data=data)+geom_line(aes(x=X,y=length),color="blue")+
  geom_line(aes(x=X,y=lcop1),color="green")+geom_line(aes(x=X,y=lcop2),color="red")
ggplot(data=data)+geom_line(aes(x=X,y=length),color="blue")+
  geom_line(aes(x=X,y=fbar),color="green")+geom_line(aes(x=X,y=cumf),color="red")
ggplot(data=data)+
  geom_line(aes(x=X,y=recr),color="green")+geom_line(aes(x=X,y=totaln),color="red")
ggplot(data=data)+geom_line(aes(x=X,y=length),color="blue")+
  geom_line(aes(x=X,y=sst),color="green")+geom_line(aes(x=X,y=sal),color="red")
ggplot(data=data)+geom_line(aes(x=X,y=length),color="blue")+geom_line(aes(x=X,y=nao),color="red")

ggplot(data=data)+geom_point(aes(x=X,y=length),color="blue")+geom_smooth()
ggplot(data=data,aes(x=X,y=length))+geom_line() +ggtitle("Zmiany długości śledzia w badanym okresie")
ggplot(data=data)+geom_density(aes(x=length),fill="blue") +ggtitle("Rozkład gęstości długości śledzia")
ggplot(data=data)+geom_bar(aes(x=xmonth))  +xlab("Miesiąc") + ylab("Liczba obserwacji")+ ggtitle("Rozkład ilości obserwacji względem miesięcy")
```

# Korelacja w zbiorze danych
Atrybuty skorelowane zostaną usunięte ze zbioru danych.
W tym przypadku będą to: lcop1,lcop2,fbar. Ponadto w żadnym modelu nie będzie uwzględniania liczba porządkowa obserwacji,
czyli X.
```{r correlations, cache=TRUE}
#X+length+cfin1+cfin2+chel1+chel2+lcop1+lcop2+fbar+recr+cumf+totaln+sst+sal+xmonth+nao
corelationsData<-round(cor(data[,3:16,drop=T], method="pearson") ,2)
str(data)
corSample<- data[sample(1:nrow(data), 500, replace=FALSE),] 
pairs(corSample)

corelationsData<-cbind(corelationsData,rownames(corelationsData))
corelationsData<-as.data.frame(corelationsData)
corelationsData<-corelationsData %>% gather (AttributeName,Corelation,1:14)
ggplot(data = corelationsData, aes(x=V15, y=AttributeName, fill = Corelation))+
 geom_tile(color = "white")+
  theme_minimal()+ 
 theme(axis.text.x = element_text(angle = 45, vjust = 1, 
    size = 12, hjust = 1))+
 coord_fixed() + ggtitle("Korelacja wśród zmiennych") + xlab("Zmienna")+ylab("Zmienna")

corelationsData<-corelationsData[corelationsData$Corelation>=0.7 & corelationsData$AttributeName!=corelationsData$V15,]
colnames(corelationsData)<-c("Atrybut","Atrybut skorelowany","Współczynnik korelacji")
kable(corelationsData)
data <-data %>% select(c(-lcop1,-lcop2,-fbar))
```

# Podział zbioru danych na dane testowe i dane treningowe
```{r datasetSplit, cache=TRUE}
set.seed(23)
#split=sample.split(data$length,SplitRatio = 0.75)
#training_set<-subset(data,split==TRUE)
#test_set<-subset(data,split==FALSE)

partitionIndexes <- 
    createDataPartition(
        # atrybut do stratyfikacji
        y = data$length,
        # procent w zbiorze uczącym
        p = .70,
        # chcemy indeksy a nie listę
        list = FALSE)

training_set <- data[ partitionIndexes,]
test_set  <- data[-partitionIndexes,]
```

# Normalizacja danych w zbiorze
```{r normalization, cache=TRUE}
set.seed(23)
#training_set[,c(10,12)]<-scale(training_set[,c(10,12)])
#test_set[,c(10,12)]<-scale(test_set[,c(10,12)])
#colnames(training_set)
```

# Ustalenie zestawu zmiennych objaśnianych z wykorzystaniem podejścia opartym na Backword Elimination
```{r mlr, cache=TRUE}
set.seed(23)
mlregressor_One<-lm(formula = length~cfin1+cfin2+chel1+chel2+recr+cumf+totaln+sst+sal+xmonth+nao
                ,data = training_set)
summary(mlregressor_One)
Y_predictions<-predict(mlregressor_One,newdata=test_set)
```

## Eliminacja zmiennej: xmonth
```{r mlr_xmonth, cache=TRUE}
set.seed(23)
mlregressor_two<-lm(formula = length~cfin2+chel1+chel2+recr+cumf+totaln+sst+sal+nao
                ,data = training_set)

Y_predictions<-predict(mlregressor_two,newdata=test_set)
paste("Prognoza na podstawie modelu")
ggplot(data=test_set)+geom_point(aes(x=X,y=length))+geom_line(aes(x=X,y=Y_predictions),color="red")

summary(mlregressor_two)
varImp(mlregressor_two)
```

## Eliminacja zmiennej: cfin2
```{r mlr_cfin2, cache=TRUE}
set.seed(23)
mlregressor_three<-lm(formula = length~chel1+chel2+recr+cumf+totaln+sst+sal+nao
                ,data = training_set)

Y_predictions<-predict(mlregressor_three,newdata=test_set)
paste("Prognoza na podstawie modelu")
ggplot(data=test_set)+geom_point(aes(x=X,y=length))+geom_line(aes(x=X,y=Y_predictions),color="red")

summary(mlregressor_three)
varImp(mlregressor_three)
```


# Gradient Boosting
```{r gbm, cache=TRUE}
set.seed(23)
fitControl <- trainControl(## 10-fold CV
                           method = "repeatedcv",
                           number = 5,
                           ## repeated ten times
                           repeats = 2)
gbmFit <- train(length ~ cfin1+cfin2+chel1+chel2+recr+cumf+totaln+sst+sal+xmonth+nao, 
                 data = training_set, 
                 method = "gbm", 
                 trControl = fitControl,
                 preProcess=c("scale","center"),
                 ## This last option is actually one
                 ## for gbm() that passes through
                 verbose = FALSE)

Y_predictions<-predict(gbmFit,newdata=test_set)
paste("Prognoza na podstawie modelu")
ggplot(data=test_set)+geom_point(aes(x=X,y=length))+geom_line(aes(x=X,y=Y_predictions),color="red")
gbmFit
summary(gbmFit)
varImp(gbmFit)
```

# Support Vector Regression
```{r svr, cache=TRUE}
set.seed(23)
# svm_regressor<-svm(formula= length ~cfin1+cfin2+chel1+chel2+recr+cumf+totaln+sst+sal+xmonth+nao,
#                    data = data,
#                    type='eps-regression')
# summary(svm_regressor)

svm_regressor<-svm(formula= length ~cfin1+cfin2+chel1+chel2+recr+cumf+totaln+sst+sal+xmonth+nao, 
                   data = training_set,
                   type='eps-regression')
svm_regressor
summary(svm_regressor)
varImp(svm_regressor)
Y_predictions <- predict(svm_regressor, newdata = test_set)
paste("Prognoza na podstawie modelu")
ggplot(data=test_set)+geom_point(aes(x=X,y=length))+geom_line(aes(x=X,y=Y_predictions),color="red")

# tuneResult <- tune.svm(length ~cfin1+cfin2+chel1+chel2+recr+cumf+totaln+sst+sal+xmonth+nao, data = training_set,
#                 cost = (1:4), gamma = seq(0,3,0.5),epsilon = seq(0,1,0.2), kernel = "radial") 
# tunedModel <- tuneResult$best.model
# tunedModel <- predict(tunedModel, test_set) 
control <- trainControl(method="repeatedcv", number=5, repeats=2)
fit.svmRadial <- train(length ~cfin1+cfin2+chel1+chel2+recr+cumf+totaln+sst+sal+xmonth+nao,
                       data=training_set, 
                       method="svmRadial", 
                       preProc=c("center", "scale"), 
                       trControl=control)

tuneResult <- tune(svm, length ~cfin1+chel1+chel2+recr+cumf+totaln+sst+sal,  data = training_set,ranges = list(epsilon = seq(0,1,0.5), cost= 1:4))


#obj <- tune.svm(length ~cfin1+chel1+chel2+recr+cumf+totaln+sst+xmonth+sal, data = training_set, cost = 2^(1:4), gamma = seq(0,3,0.5),epsilon = seq(0,1,0.1), kernel = "radial") 
#obj$best.performance

#tuneResult <- tune(svm,length ~cfin1+chel1+chel2+recr+cumf+totaln+sst+xmonth+sal,  data = training_set,
 #             ranges = list(cost = 2^(1:4), gamma = seq(0,3,0.5),epsilon = seq(0,1,0.1)))
#tunedModel <- tuneResult$best.model

# resamps <- resamples(list(GBM = gbmFit3,
#                           SVM = svmFit,
#                           RDA = rdaFit))
# resamps
  
```

# Random Forest
```{r randomForest, cache=TRUE}
set.seed(23)
#reg1<-regsubsets(length ~ cfin1+cfin2+chel1+chel2+recr+cumf+totaln+sst+sal+nao,data = training_set, nvmax = 8)
#summary(reg1)
ctrl <- trainControl(
    # powtórzona ocena krzyżowa
    method = "repeatedcv",
    # liczba podziałów
    number = 2,
    # liczba powtórzeń
    repeats = 5)
RF_fit <- train(length ~ cfin1+cfin2+chel1+chel2+recr+cumf+totaln+sst+sal+xmonth+nao,
             data = training_set,
             method = "rf",
             trControl = ctrl,
             preProcess=c("center","scale"),
             importance=T,
             # Paramter dla algorytmu uczącego
             ntree = 50)
RF_fit
summary(RF_fit)
varImp(RF_fit)

Y_predictions <- predict(RF_fit, newdata = test_set)
paste("Prognoza na podstawie modelu")
ggplot(data=test_set)+geom_point(aes(x=X,y=length))+geom_line(aes(x=X,y=Y_predictions),color="red")
 
```


# Regresja za pomocą drzewa decyzyjnego
```{r decisionTree, cache=TRUE}
set.seed(23)
regressor<-rpart(formula = length ~ cfin1+cfin2+chel1+chel2+recr+cumf+totaln+sst+sal+xmonth+nao
                    , data=training_set
                    , control = rpart.control(minsplit = 1000))
regressor
summary(regressor)
Y_predictions <- predict(regressor, newdata = test_set)
ggplot(data=test_set)+geom_point(aes(x=X,y=length))+geom_line(aes(x=X,y=Y_predictions),color="red")
varImp(regressor)
```

